# -*- coding: utf-8 -*-
"""Actor-Critic:

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lmZ-UZoMZ1huL8svkzDUKU6sQg01XIcS
"""

'''
import library:
'''
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Lambda
import numpy as np
import gym
import wandb

'''
# Initialize WandB ( project name)-- basiyan model
wandb.init(name='A2C', project="deep-rl-tf2")
'''

# Hyperparameters
gamma = 0.99
update_interval = 5
actor_lr = 0.0005
critic_lr = 0.001

# Set float precision
tf.keras.backend.set_floatx('float64')

"""Actor Network:"""

# Actor Network
class Actor:
    def __init__(self, state_dim, action_dim, action_bound, std_bound):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.action_bound = action_bound
        self.std_bound = std_bound
        self.model = self.create_model()
        self.opt = tf.keras.optimizers.Adam(actor_lr)

    def create_model(self):
        state_input = Input((self.state_dim,))
        dense_1 = Dense(32, activation='relu')(state_input)
        dense_2 = Dense(32, activation='relu')(dense_1)
        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)
        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)
        std_output = Dense(self.action_dim, activation='softplus')(dense_2)
        return tf.keras.models.Model(state_input, [mu_output, std_output])

    def get_action(self, state):
        state = np.reshape(state, [1, self.state_dim])
        mu, std = self.model.predict(state, verbose=0)
        mu, std = mu[0], std[0]
        action = np.random.normal(mu, std, size=self.action_dim)
        return np.clip(action, -self.action_bound, self.action_bound)

    def log_pdf(self, mu, std, action):
        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])
        var = std ** 2
        log_policy_pdf = -0.5 * ((action - mu) ** 2 / var + tf.math.log(var * 2 * np.pi))
        return tf.reduce_sum(log_policy_pdf, axis=1, keepdims=True)

    def compute_loss(self, mu, std, actions, advantages):
        log_policy_pdf = self.log_pdf(mu, std, actions)
        loss_policy = log_policy_pdf * advantages
        return -tf.reduce_mean(loss_policy)

    def train(self, states, actions, advantages):
        with tf.GradientTape() as tape:
            mu, std = self.model(states, training=True)
            loss = self.compute_loss(mu, std, actions, advantages)
        grads = tape.gradient(loss, self.model.trainable_variables)
        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))
        return loss

"""Critic Network:"""

# Critic Network
class Critic:
    def __init__(self, state_dim):
        self.state_dim = state_dim
        self.model = self.create_model()
        self.opt = tf.keras.optimizers.Adam(critic_lr)

    def create_model(self):
        return tf.keras.Sequential([
            Input((self.state_dim,)),
            Dense(32, activation='relu'),
            Dense(32, activation='relu'),
            Dense(16, activation='relu'),
            Dense(1, activation='linear')
        ])

    def compute_loss(self, v_pred, td_targets):
        mse = tf.keras.losses.MeanSquaredError()
        return mse(td_targets, v_pred)

    def train(self, states, td_targets):
        with tf.GradientTape() as tape:
            v_pred = self.model(states, training=True)
            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))
        grads = tape.gradient(loss, self.model.trainable_variables)
        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))
        return loss

"""Agent:"""

# Agent
class Agent:
    def __init__(self, env):
        self.env = env
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.shape[0]
        self.action_bound = self.env.action_space.high[0]
        self.std_bound = [1e-2, 1.0]
        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound, self.std_bound)
        self.critic = Critic(self.state_dim)

    def td_target(self, reward, next_state, done):
        if done:
            return reward
        v_value = self.critic.model.predict(np.reshape(next_state, [1, self.state_dim]), verbose=0)
        return reward + gamma * v_value[0]

    def advantage(self, td_targets, baselines):
        return td_targets - baselines

    def train(self, max_episodes=1000):
        for ep in range(max_episodes):
            states, actions, td_targets, advantages = [], [], [], []
            state, done, episode_reward = self.env.reset(), False, 0

            while not done:
                action = self.actor.get_action(state)
                next_state, reward, done, _ = self.env.step(action)

                td_target = self.td_target(reward, next_state, done)
                baseline = self.critic.model.predict(np.reshape(state, [1, self.state_dim]), verbose=0)
                advantage = self.advantage(td_target, baseline)

                states.append(state)
                actions.append(action)
                td_targets.append(td_target)
                advantages.append(advantage)

                if len(states) >= update_interval or done:
                    states = np.vstack(states)
                    actions = np.vstack(actions)
                    td_targets = np.vstack(td_targets)
                    advantages = np.vstack(advantages)

                    self.actor.train(states, actions, advantages)
                    self.critic.train(states, td_targets)

                    states, actions, td_targets, advantages = [], [], [], []

                episode_reward += reward
                state = next_state

            print(f"Episode {ep}: Reward = {episode_reward}")
            wandb.log({'Reward': episode_reward})

"""Main Function"""

wandb.init(name='A2C', project="deep-rl-tf2")
# Main Function
def main():
    env_name = 'Pendulum-v1'  # Updated for compatibility
    env = gym.make(env_name)
    agent = Agent(env)
    agent.train()

if __name__ == "__main__":
    main()